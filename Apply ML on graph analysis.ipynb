{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ed8bb6",
   "metadata": {},
   "source": [
    "# svm using linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4bbaaa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AverageShortestPathLength</th>\n",
       "      <th>BetweennessCentrality</th>\n",
       "      <th>ClosenessCentrality</th>\n",
       "      <th>ClusteringCoefficient</th>\n",
       "      <th>Degree</th>\n",
       "      <th>TopologicalCoefficient</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.640046</td>\n",
       "      <td>0.036598</td>\n",
       "      <td>0.115740</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>21</td>\n",
       "      <td>0.102381</td>\n",
       "      <td>SCN2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.699074</td>\n",
       "      <td>0.023132</td>\n",
       "      <td>0.149274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>HTR2B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.625000</td>\n",
       "      <td>0.028912</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>HTR2C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.079861</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>0.141246</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>23</td>\n",
       "      <td>0.106317</td>\n",
       "      <td>HTR1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.114583</td>\n",
       "      <td>0.015466</td>\n",
       "      <td>0.140556</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>22</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>HTR1B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.109954</td>\n",
       "      <td>0.014929</td>\n",
       "      <td>0.140648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.168182</td>\n",
       "      <td>HTR1F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.171296</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>0.139445</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>20</td>\n",
       "      <td>0.087179</td>\n",
       "      <td>HTR1E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.876157</td>\n",
       "      <td>0.055609</td>\n",
       "      <td>0.126965</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>21</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>AOC3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.104167</td>\n",
       "      <td>0.107686</td>\n",
       "      <td>0.140762</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>21</td>\n",
       "      <td>0.082692</td>\n",
       "      <td>HIF1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.516204</td>\n",
       "      <td>0.033078</td>\n",
       "      <td>0.133046</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>21</td>\n",
       "      <td>0.078571</td>\n",
       "      <td>EGLN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.612269</td>\n",
       "      <td>0.061830</td>\n",
       "      <td>0.151234</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>23</td>\n",
       "      <td>0.122995</td>\n",
       "      <td>DHRS4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.863426</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>22</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>RDH8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.134259</td>\n",
       "      <td>0.210707</td>\n",
       "      <td>0.163019</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>20</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>RDH14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.983796</td>\n",
       "      <td>0.017078</td>\n",
       "      <td>0.143189</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>LRAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.601852</td>\n",
       "      <td>0.043523</td>\n",
       "      <td>0.131547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>RDH13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.135417</td>\n",
       "      <td>0.105439</td>\n",
       "      <td>0.162988</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>20</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>ALDH1A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.135417</td>\n",
       "      <td>0.105655</td>\n",
       "      <td>0.162988</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>20</td>\n",
       "      <td>0.110417</td>\n",
       "      <td>ALDH1A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.594907</td>\n",
       "      <td>0.055045</td>\n",
       "      <td>0.151632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>APOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.094907</td>\n",
       "      <td>0.130184</td>\n",
       "      <td>0.164071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>PTGDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.047454</td>\n",
       "      <td>0.092069</td>\n",
       "      <td>0.165359</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>21</td>\n",
       "      <td>0.111472</td>\n",
       "      <td>GLP1R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.630787</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>0.150812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.168519</td>\n",
       "      <td>HRH2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.733796</td>\n",
       "      <td>0.049319</td>\n",
       "      <td>0.148505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>PLA2G1B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.498843</td>\n",
       "      <td>0.070906</td>\n",
       "      <td>0.153874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>LTC4S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.975000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>fabI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.608796</td>\n",
       "      <td>0.262476</td>\n",
       "      <td>0.178291</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>22</td>\n",
       "      <td>0.054588</td>\n",
       "      <td>TFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.858796</td>\n",
       "      <td>0.244066</td>\n",
       "      <td>0.170684</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>21</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>FTH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.285880</td>\n",
       "      <td>0.039943</td>\n",
       "      <td>0.120687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>HDAC8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.723380</td>\n",
       "      <td>0.034370</td>\n",
       "      <td>0.129477</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>21</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>AHSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.724537</td>\n",
       "      <td>0.034315</td>\n",
       "      <td>0.129458</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>20</td>\n",
       "      <td>0.084091</td>\n",
       "      <td>HBA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.045139</td>\n",
       "      <td>0.043523</td>\n",
       "      <td>0.124299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>FXN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.792824</td>\n",
       "      <td>0.058315</td>\n",
       "      <td>0.113729</td>\n",
       "      <td>0.063241</td>\n",
       "      <td>23</td>\n",
       "      <td>0.089130</td>\n",
       "      <td>FEN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9.737269</td>\n",
       "      <td>0.020708</td>\n",
       "      <td>0.102698</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>22</td>\n",
       "      <td>0.102222</td>\n",
       "      <td>NEIL1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9.737269</td>\n",
       "      <td>0.022979</td>\n",
       "      <td>0.102698</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>22</td>\n",
       "      <td>0.101136</td>\n",
       "      <td>NEIL2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8.796296</td>\n",
       "      <td>0.051280</td>\n",
       "      <td>0.113684</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>22</td>\n",
       "      <td>0.101111</td>\n",
       "      <td>POLB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.137731</td>\n",
       "      <td>0.075748</td>\n",
       "      <td>0.162927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>CP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.540509</td>\n",
       "      <td>0.238164</td>\n",
       "      <td>0.180489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.615741</td>\n",
       "      <td>0.245208</td>\n",
       "      <td>0.178071</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>20</td>\n",
       "      <td>0.057813</td>\n",
       "      <td>DPP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.194245</td>\n",
       "      <td>0.169965</td>\n",
       "      <td>0.313063</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>20</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>mrdA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.956835</td>\n",
       "      <td>0.245296</td>\n",
       "      <td>0.338200</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>20</td>\n",
       "      <td>0.184091</td>\n",
       "      <td>mrcB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.467626</td>\n",
       "      <td>0.098989</td>\n",
       "      <td>0.288382</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>20</td>\n",
       "      <td>0.207895</td>\n",
       "      <td>mrcA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>pbp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.834532</td>\n",
       "      <td>0.324184</td>\n",
       "      <td>0.352792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>ftsI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3.453237</td>\n",
       "      <td>0.140678</td>\n",
       "      <td>0.289583</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>20</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>dacB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3.503597</td>\n",
       "      <td>0.104110</td>\n",
       "      <td>0.285421</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>20</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>dacC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6.983796</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>0.143189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.132692</td>\n",
       "      <td>DRD1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8.233796</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.121451</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21</td>\n",
       "      <td>0.099567</td>\n",
       "      <td>KCNJ1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.975000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>ydbK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nfsA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>rpsJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8.285880</td>\n",
       "      <td>0.038544</td>\n",
       "      <td>0.120687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>VDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.283565</td>\n",
       "      <td>0.064503</td>\n",
       "      <td>0.137295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>TPO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>7.695602</td>\n",
       "      <td>0.045193</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>20</td>\n",
       "      <td>0.106522</td>\n",
       "      <td>SLC12A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.221065</td>\n",
       "      <td>0.027643</td>\n",
       "      <td>0.121639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.131250</td>\n",
       "      <td>SLC12A2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AverageShortestPathLength  BetweennessCentrality  ClosenessCentrality  \\\n",
       "0                    8.640046               0.036598             0.115740   \n",
       "1                    6.699074               0.023132             0.149274   \n",
       "2                    6.625000               0.028912             0.150943   \n",
       "3                    7.079861               0.022103             0.141246   \n",
       "4                    7.114583               0.015466             0.140556   \n",
       "5                    7.109954               0.014929             0.140648   \n",
       "6                    7.171296               0.024206             0.139445   \n",
       "7                    7.876157               0.055609             0.126965   \n",
       "8                    7.104167               0.107686             0.140762   \n",
       "9                    7.516204               0.033078             0.133046   \n",
       "10                   6.612269               0.061830             0.151234   \n",
       "11                   6.863426               0.013896             0.145700   \n",
       "12                   6.134259               0.210707             0.163019   \n",
       "13                   6.983796               0.017078             0.143189   \n",
       "14                   7.601852               0.043523             0.131547   \n",
       "15                   6.135417               0.105439             0.162988   \n",
       "16                   6.135417               0.105655             0.162988   \n",
       "17                   6.594907               0.055045             0.151632   \n",
       "18                   6.094907               0.130184             0.164071   \n",
       "19                   6.047454               0.092069             0.165359   \n",
       "20                   6.630787               0.020072             0.150812   \n",
       "21                   6.733796               0.049319             0.148505   \n",
       "22                   6.498843               0.070906             0.153874   \n",
       "23                   1.975000               0.730769             0.506329   \n",
       "24                   5.608796               0.262476             0.178291   \n",
       "25                   5.858796               0.244066             0.170684   \n",
       "26                   8.285880               0.039943             0.120687   \n",
       "27                   7.723380               0.034370             0.129477   \n",
       "28                   7.724537               0.034315             0.129458   \n",
       "29                   8.045139               0.043523             0.124299   \n",
       "30                   8.792824               0.058315             0.113729   \n",
       "31                   9.737269               0.020708             0.102698   \n",
       "32                   9.737269               0.022979             0.102698   \n",
       "33                   8.796296               0.051280             0.113684   \n",
       "34                   6.137731               0.075748             0.162927   \n",
       "35                   5.540509               0.238164             0.180489   \n",
       "36                   5.615741               0.245208             0.178071   \n",
       "37                   3.194245               0.169965             0.313063   \n",
       "38                   2.956835               0.245296             0.338200   \n",
       "39                   3.467626               0.098989             0.288382   \n",
       "40                   1.000000               1.000000             1.000000   \n",
       "41                   2.834532               0.324184             0.352792   \n",
       "42                   3.453237               0.140678             0.289583   \n",
       "43                   3.503597               0.104110             0.285421   \n",
       "44                   6.983796               0.029030             0.143189   \n",
       "45                   8.233796               0.025126             0.121451   \n",
       "46                   1.975000               0.730769             0.506329   \n",
       "47                   1.000000               1.000000             1.000000   \n",
       "48                   1.000000               1.000000             1.000000   \n",
       "49                   8.285880               0.038544             0.120687   \n",
       "50                   7.283565               0.064503             0.137295   \n",
       "51                   7.695602               0.045193             0.129944   \n",
       "52                   8.221065               0.027643             0.121639   \n",
       "\n",
       "    ClusteringCoefficient  Degree  TopologicalCoefficient         name  \n",
       "0                0.036842      21                0.102381        SCN2A  \n",
       "1                0.000000      20                0.184211        HTR2B  \n",
       "2                0.000000      21                0.161905        HTR2C  \n",
       "3                0.077922      23                0.106317        HTR1A  \n",
       "4                0.057143      22                0.137566        HTR1B  \n",
       "5                0.000000      20                0.168182        HTR1F  \n",
       "6                0.031579      20                0.087179        HTR1E  \n",
       "7                0.010526      21                0.068182         AOC3  \n",
       "8                0.031579      21                0.082692        HIF1A  \n",
       "9                0.031579      21                0.078571        EGLN1  \n",
       "10               0.021645      23                0.122995        DHRS4  \n",
       "11               0.057143      22                0.173016         RDH8  \n",
       "12               0.015789      20                0.092857        RDH14  \n",
       "13               0.100000      20                0.159091       LRAT    \n",
       "14               0.000000      20                0.050000     RDH13     \n",
       "15               0.036842      20                0.112500    ALDH1A2    \n",
       "16               0.036842      20                0.110417      ALDH1A1  \n",
       "17               0.000000      20                0.100000         APOD  \n",
       "18               0.000000      20                0.057143        PTGDS  \n",
       "19               0.009524      21                0.111472        GLP1R  \n",
       "20               0.000000      20                0.168519         HRH2  \n",
       "21               0.000000      20                0.100000      PLA2G1B  \n",
       "22               0.000000      20                0.062500        LTC4S  \n",
       "23               0.000000      20                0.050000         fabI  \n",
       "24               0.014286      22                0.054588         TFRC  \n",
       "25               0.015789      21                0.062500         FTH1  \n",
       "26               0.000000      20                0.075000        HDAC8  \n",
       "27               0.033333      21                0.079365         AHSP  \n",
       "28               0.036842      20                0.084091       HBA1    \n",
       "29               0.000000      20                0.050000          FXN  \n",
       "30               0.063241      23                0.089130         FEN1  \n",
       "31               0.110526      22                0.102222        NEIL1  \n",
       "32               0.100000      22                0.101136        NEIL2  \n",
       "33               0.105263      22                0.101111         POLB  \n",
       "34               0.000000      20                0.109091         CP    \n",
       "35               0.000000      20                0.070000           TF  \n",
       "36               0.010526      20                0.057813         DPP4  \n",
       "37               0.026316      20                0.164286       mrdA    \n",
       "38               0.031579      20                0.184091         mrcB  \n",
       "39               0.036842      20                0.207895        mrcA   \n",
       "40               0.000000      20                0.000000         pbp4  \n",
       "41               0.000000      20                0.169231        ftsI   \n",
       "42               0.015789      20                0.152778      dacB     \n",
       "43               0.052632      20                0.216667         dacC  \n",
       "44               0.000000      20                0.132692         DRD1  \n",
       "45               0.047619      21                0.099567        KCNJ1  \n",
       "46               0.000000      20                0.050000        ydbK   \n",
       "47               0.000000      20                0.000000         nfsA  \n",
       "48               0.000000      20                0.000000         rpsJ  \n",
       "49               0.000000      20                0.087500          VDR  \n",
       "50               0.000000      20                0.050000          TPO  \n",
       "51               0.052632      20                0.106522    SLC12A1    \n",
       "52               0.000000      20                0.131250      SLC12A2  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('main.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0dc971f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> features, y -> label\n",
    "#here we will not use gender and user id\n",
    "#on the basis of age and estimated salary we will decide the person will purchase the product or not\n",
    "x=df.iloc[:,[0]]# that's why in x we have 0th no column \n",
    "y=df.iloc[:,4]#and in y we have column no 4 that we will predict means dependent variable\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3db44aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AverageShortestPathLength\n",
      "0                   8.640046\n",
      "1                   6.699074\n",
      "2                   6.625000\n",
      "3                   7.079861\n",
      "4                   7.114583\n"
     ]
    }
   ],
   "source": [
    "print(x.head())\n",
    "#.head() is used to predict first five values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2f27c84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21\n",
      "1    20\n",
      "2    21\n",
      "3    23\n",
      "4    22\n",
      "Name: Degree, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "74217b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now devide or split the data for training and testing the set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
    "#means 25% data will consider as test data and remaining data will consider as tarining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "79ca05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 1)\n",
      "(14, 1)\n"
     ]
    }
   ],
   "source": [
    "#check the shape of traing and testing data\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "#means we will train our model in the basis of first 300 row of that 2 colummn age and estimated salary\n",
    "#and we will test  our model in the basis of last remaining 100 row of that 2 colummn age and estimated salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "52ca4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now scale or feature or preprocessing the data using standard sccaler we can use minmax scaler too for scalling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x=StandardScaler()\n",
    "x_train=sc_x.fit_transform(x_train)\n",
    "x_test=sc_x.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4059a4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply svm algo for classification\n",
    "#by finding the right hyperplane that classifies the datapoint in the graph\n",
    "from sklearn.svm import SVC\n",
    "#in svm we have many kernal like linear,sigmoid,polynomial,rbf we usde here linear\n",
    "classifier = SVC(kernel ='linear',random_state=0)\n",
    "classifier.fit(x_train,y_train)#here we train pour alogorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aa274cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=classifier.predict(x_test)\n",
    "y_pred\n",
    "#here we will have 100 data means y value for 100 x test values\n",
    "#these are the predicted value in boolean format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2c410bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.857142857142854 %\n",
      "F1 score: 25.71428571428571 %\n",
      "recall: 42.857142857142854 %\n",
      "precision: 42.857142857142854 %\n"
     ]
    }
   ],
   "source": [
    " # Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred )*100,\"%\")\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, y_pred, average='weighted')*100,\"%\")\n",
    "print(\"recall:\",metrics.recall_score(y_test, y_pred, average='weighted')*100,\"%\")\n",
    "print(\"precision:\",metrics.precision_score(y_test, y_pred, average='micro')*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da5055",
   "metadata": {},
   "source": [
    "# svm with rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5db85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bc266270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now devide or split the data for training and testing the set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.50,random_state=0)\n",
    "#means 25% data will consider as test data and remaining data will consider as tarining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1b2e0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now scale or feature or preprocessing the data using standard sccaler we can use minmax scaler too for scalling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x=StandardScaler()\n",
    "x_train=sc_x.fit_transform(x_train)\n",
    "x_test=sc_x.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "553c479e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply svm algo for classification\n",
    "#by finding the right hyperplane that classifies the datapoint in the graph\n",
    "from sklearn.svm import SVC\n",
    "#in svm we have many kernal like linear,sigmoid,polynomial,rbf we usde here rbf\n",
    "classifier = SVC(kernel ='rbf')\n",
    "classifier.fit(x_train,y_train)#here we train pour alogorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d766d283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20], dtype=int64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=classifier.predict(x_test)\n",
    "y_pred\n",
    "#here we will have 100 data means y value for 100 x test values\n",
    "#these are the predicted value in boolean format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7666997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.96296296296296 %\n",
      "F1 score: 48.65319865319865 %\n",
      "recall: 62.96296296296296 %\n",
      "precision: 62.96296296296296 %\n"
     ]
    }
   ],
   "source": [
    " # Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred )*100,\"%\")\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, y_pred, average='weighted')*100,\"%\")\n",
    "print(\"recall:\",metrics.recall_score(y_test, y_pred, average='macro')*100,\"%\")\n",
    "print(\"precision:\",metrics.precision_score(y_test, y_pred, average='micro')*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf446bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81806105",
   "metadata": {},
   "source": [
    "# svm using polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "194d5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now devide or split the data for training and testing the set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
    "#means 25% data will consider as test data and remaining data will consider as tarining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9f483bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now scale or feature or preprocessing the data using standard sccaler we can use minmax scaler too for scalling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x=StandardScaler()\n",
    "x_train=sc_x.fit_transform(x_train)\n",
    "x_test=sc_x.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0f20cabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(degree=4, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(degree=4, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(degree=4, kernel='poly')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply svm algo for classification\n",
    "#by finding the right hyperplane that classifies the datapoint in the graph\n",
    "from sklearn.svm import SVC\n",
    "#in svm we have many kernal like linear,sigmoid,polynomial,rbf we usde here rbf\n",
    "ddd = SVC(kernel ='poly',degree=4)\n",
    "ddd.fit(x_train,y_train)#here we train pour alogorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "95d17804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=ddd.predict(x_test)\n",
    "y_pred\n",
    "#here we will have 100 data means y value for 100 x test values\n",
    "#these are the predicted value in boolean format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "223293fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.857142857142854 %\n",
      "F1 score: 25.71428571428571 %\n",
      "recall: 25.0 %\n",
      "precision: 42.857142857142854 %\n"
     ]
    }
   ],
   "source": [
    " # Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred )*100,\"%\")\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, y_pred, average='weighted')*100,\"%\")\n",
    "print(\"recall:\",metrics.recall_score(y_test, y_pred, average='macro')*100,\"%\")\n",
    "print(\"precision:\",metrics.precision_score(y_test, y_pred, average='micro')*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62424f3",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a2e2eca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.53407156, 21.07980787, 20.52038429, 20.52757888, 20.05180651,\n",
       "       20.12924907, 20.08780094, 20.60794776, 20.85975857, 20.64409621,\n",
       "       20.7744763 ])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing set (80/20)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 28)\n",
    "# Initializing the Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "# Fitting the Multiple Linear Regression model to the data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f1dbf59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of the model is 32.02 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "Accuracy=r2_score(y_test,y_pred)*100\n",
    "print(\" Accuracy of the model is %.2f\" %Accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f2e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e155f6be",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "20d35888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 22 20 20 20 20 20 20 20 20 20]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "x=df.iloc[:,[0]]# that's why in x we have 0th no column \n",
    "y=df.iloc[:,4]#and in y we have column no 4 that we will predict means dependent variable\n",
    "  \n",
    "    #now devide or split the data for training and testing the set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=28)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "y_pred2 = dt.predict(x_test)\n",
    "print(y_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b4c4ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.63636363636363 %\n",
      "F1 score: 47.222222222222214 %\n",
      "recall: 50.0 %\n",
      "precision: 63.63636363636363 %\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred2 )*100,\"%\")\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, y_pred2, average='macro')*100,\"%\")\n",
    "print(\"recall:\",metrics.recall_score(y_test, y_pred2, average='macro')*100,\"%\")\n",
    "print(\"precision:\",metrics.precision_score(y_test, y_pred2, average='micro')*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662397f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
